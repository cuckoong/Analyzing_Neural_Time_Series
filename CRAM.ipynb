{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3EXKAuLkJkVi"
      ],
      "authorship_tag": "ABX9TyNit3PYMFiiLDngk42M6JSa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cuckoong/Analyzing_Neural_Time_Series/blob/master/CRAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_pLYzEw8vIW",
        "colab_type": "text"
      },
      "source": [
        "## setup environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Douec7lz4dI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5533e10f-5cb5-4d03-e803-7fd3ea080123"
      },
      "source": [
        "## using tensorflow 1.14\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNNioecGI2Aq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "# from cnn_class import cnn\n",
        "import time\n",
        "import scipy.io as sio\n",
        "from sklearn.metrics import classification_report, roc_auc_score, auc, roc_curve, f1_score\n",
        "# from RnnAttention.attention import attention\n",
        "from scipy import interp\n",
        "# import tensorflow.compat.v1 as tf\n",
        "# tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EXKAuLkJkVi",
        "colab_type": "text"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2-df5yeJiy4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def attention(inputs, attention_size, time_major=False, return_alphas=False, train_phase=True):\n",
        "    \"\"\"\n",
        "    Attention mechanism layer which reduces RNN/Bi-RNN outputs with Attention vector.\n",
        "    \n",
        "    Args:\n",
        "        inputs: The Attention inputs.\n",
        "            Matches outputs of RNN/Bi-RNN layer (not final state):\n",
        "                In case of RNN, this must be RNN outputs `Tensor`:\n",
        "                    If time_major == False (default), this must be a tensor of shape:\n",
        "                        `[batch_size, max_time, cell.output_size]`.\n",
        "                    If time_major == True, this must be a tensor of shape:\n",
        "                        `[max_time, batch_size, cell.output_size]`.\n",
        "                In case of Bidirectional RNN, this must be a tuple (outputs_fw, outputs_bw) containing the forward and\n",
        "                the backward RNN outputs `Tensor`.\n",
        "                    If time_major == False (default),\n",
        "                        outputs_fw is a `Tensor` shaped:\n",
        "                        `[batch_size, max_time, cell_fw.output_size]`\n",
        "                        and outputs_bw is a `Tensor` shaped:\n",
        "                        `[batch_size, max_time, cell_bw.output_size]`.\n",
        "                    If time_major == True,\n",
        "                        outputs_fw is a `Tensor` shaped:\n",
        "                        `[max_time, batch_size, cell_fw.output_size]`\n",
        "                        and outputs_bw is a `Tensor` shaped:\n",
        "                        `[max_time, batch_size, cell_bw.output_size]`.\n",
        "        attention_size: Linear size of the Attention weights.\n",
        "        time_major: The shape format of the `inputs` Tensors.\n",
        "            If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n",
        "            If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n",
        "            Using `time_major = True` is a bit more efficient because it avoids\n",
        "            transposes at the beginning and end of the RNN calculation.  However,\n",
        "            most TensorFlow data is batch-major, so by default this function\n",
        "            accepts input and emits output in batch-major form.\n",
        "        return_alphas: Whether to return attention coefficients variable along with layer's output.\n",
        "            Used for visualization purpose.\n",
        "    Returns:\n",
        "        The Attention output `Tensor`.\n",
        "        In case of RNN, this will be a `Tensor` shaped:\n",
        "            `[batch_size, cell.output_size]`.\n",
        "        In case of Bidirectional RNN, this will be a `Tensor` shaped:\n",
        "            `[batch_size, cell_fw.output_size + cell_bw.output_size]`.\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(inputs, tuple):\n",
        "        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n",
        "        inputs = tf.concat(inputs, 2)\n",
        "\n",
        "    if time_major:\n",
        "        # (T,B,D) => (B,T,D)\n",
        "        inputs = tf.array_ops.transpose(inputs, [1, 0, 2])\n",
        "\n",
        "    hidden_size = inputs.shape[2].value  # D value - hidden size of the RNN layer\n",
        "\n",
        "    # Trainable parameters\n",
        "    w_omega = tf.Variable(tf.random_normal([hidden_size, attention_size], stddev=0.1))\n",
        "    b_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
        "    u_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
        "\n",
        "    with tf.name_scope('v'):\n",
        "        # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n",
        "        #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
        "        v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)\n",
        "    # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n",
        "    vu = tf.tensordot(v, u_omega, axes=1, name='vu')  # (B,T) shape\n",
        "    alphas = tf.nn.softmax(vu, name='alphas')         # (B,T) shape\n",
        "\n",
        "    # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n",
        "    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)\n",
        "\n",
        "    if not return_alphas:\n",
        "        return output\n",
        "    else:\n",
        "        return output, alphas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEs-kFPcJP5B",
        "colab_type": "text"
      },
      "source": [
        "## cnn class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXXr7YZxJNv9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class cnn:\n",
        "\tdef __init__(\n",
        "\t\t\tself,\n",
        "\t\t\tweight_stddev\t= 0.1,\n",
        "\t\t\tbias_constant\t= 0.1,\n",
        "\t\t\tpadding\t\t\t= \"SAME\",\n",
        "\t\t\t):\n",
        "\t\t\tself.weight_stddev\t= weight_stddev\n",
        "\t\t\tself.bias_constant\t= bias_constant\n",
        "\t\t\tself.padding\t\t= padding\n",
        "\n",
        "\tdef weight_variable(self, shape):\n",
        "\t\tinitial = tf.truncated_normal(shape, stddev = self.weight_stddev)\n",
        "\t\treturn tf.Variable(initial)\n",
        "\n",
        "\n",
        "\tdef bias_variable(self, shape):\n",
        "\t\tinitial = tf.constant(self.bias_constant, shape = shape)\n",
        "\t\treturn tf.Variable(initial)\n",
        "\n",
        "\n",
        "\tdef conv1d(self, x, W, kernel_stride):\n",
        "\t# API: must strides[0]=strides[4]=1\n",
        "\t\treturn tf.nn.conv1d(x, W, stride=kernel_stride, padding=self.padding)\n",
        "\n",
        "\n",
        "\tdef conv2d(self, x, W, kernel_stride):\n",
        "\t# API: must strides[0]=strides[4]=1\n",
        "\t\treturn tf.nn.conv2d(x, W, strides=[1, kernel_stride, kernel_stride, 1], padding=self.padding)\n",
        "\n",
        "\n",
        "\tdef conv3d(self, x, W, kernel_stride):\n",
        "\t# API: must strides[0]=strides[4]=1\n",
        "\t\treturn tf.nn.conv3d(x, W, strides=[1, kernel_stride, kernel_stride, kernel_stride, 1], padding=self.padding)\n",
        "\n",
        "\n",
        "\tdef apply_conv1d(self, x, filter_width, in_channels, out_channels, kernel_stride, train_phase):\n",
        "\t\tweight = self.weight_variable([filter_width, in_channels, out_channels])\n",
        "\t\tbias = self.bias_variable([out_channels]) # each feature map shares the same weight and bias\n",
        "\t\tconv_1d = tf.add(self.conv1d(x, weight, kernel_stride), bias)\n",
        "\t\tconv_1d_bn = self.batch_norm_cnv_1d(conv_1d, train_phase)\n",
        "\t\treturn tf.nn.relu(conv_1d_bn)\n",
        "\n",
        "\n",
        "\tdef apply_conv2d(self, x, filter_height, filter_width, in_channels, out_channels, kernel_stride, train_phase):\n",
        "\t\tweight = self.weight_variable([filter_height, filter_width, in_channels, out_channels])\n",
        "\t\tbias = self.bias_variable([out_channels]) # each feature map shares the same weight and bias\n",
        "\t\tconv_2d = tf.add(self.conv2d(x, weight, kernel_stride), bias)\n",
        "\t\tconv_2d_bn = self.batch_norm_cnv_2d(conv_2d, train_phase)\n",
        "\t\treturn tf.nn.relu(conv_2d_bn)\n",
        "\n",
        "\n",
        "\t\n",
        "\tdef apply_conv3d(self, x, filter_depth, filter_height, filter_width, in_channels, out_channels, kernel_stride, train_phase):\n",
        "\t\tweight = self.weight_variable([filter_depth, filter_height, filter_width, in_channels, out_channels])\n",
        "\t\tbias = self.bias_variable([out_channels]) # each feature map shares the same weight and bias\n",
        "\t\tconv_3d = tf.add(self.conv3d(x, weight, kernel_stride), bias)\n",
        "\t\tconv_3d_bn = self.batch_norm_cnv_3d(conv_3d, train_phase)\n",
        "\t\treturn tf.nn.relu(conv_3d_bn)\n",
        "\n",
        "\n",
        "\tdef batch_norm_cnv_3d(self, inputs, train_phase):\n",
        "\t\treturn tf.layers.batch_normalization(inputs, axis=4, momentum=0.993, epsilon=1e-5, scale=False, training=train_phase)\n",
        "\n",
        "\n",
        "\tdef batch_norm_cnv_2d(self, inputs, train_phase):\n",
        "\t\treturn tf.layers.batch_normalization(inputs, axis=3, momentum=0.993, epsilon=1e-5, scale=False, training=train_phase)\n",
        "\n",
        "\n",
        "\tdef batch_norm_cnv_1d(self, inputs, train_phase):\n",
        "\t\treturn tf.layers.batch_normalization(inputs, axis=2, momentum=0.993, epsilon=1e-5, scale=False, training=train_phase)\n",
        "\n",
        "\n",
        "\tdef batch_norm(self, inputs, train_phase):\n",
        "\t\treturn tf.layers.batch_normalization(inputs, axis=1, momentum=0.993, epsilon=1e-5, scale=False, training=train_phase)\n",
        "\n",
        "\n",
        "\tdef apply_max_pooling(self, x, pooling_height, pooling_width, pooling_stride):\n",
        "\t# API: must ksize[0]=ksize[4]=1, strides[0]=strides[4]=1\n",
        "\t\treturn tf.nn.max_pool(x, ksize=[1, pooling_height, pooling_width, 1], strides=[1, pooling_stride, pooling_stride, 1], padding=self.padding)\n",
        "\n",
        "\n",
        "\tdef apply_max_pooling3d(self, x, pooling_depth, pooling_height, pooling_width, pooling_stride):\n",
        "\t# API: must ksize[0]=ksize[4]=1, strides[0]=strides[4]=1\n",
        "\t\treturn tf.nn.max_pool3d(x, ksize=[1, pooling_depth, pooling_height, pooling_width, 1], strides=[1, pooling_stride, pooling_stride, pooling_stride, 1], padding=self.padding)\n",
        "\n",
        "\t\n",
        "\tdef apply_fully_connect(self, x, x_size, fc_size, train_phase):\n",
        "\t\tfc_weight = self.weight_variable([x_size, fc_size])\n",
        "\t\tfc_bias = self.bias_variable([fc_size])\n",
        "\t\tfc = tf.add(tf.matmul(x, fc_weight), fc_bias)\n",
        "\t\tfc_bn = self.batch_norm(fc, train_phase)\n",
        "\t\treturn tf.nn.relu(fc_bn)\n",
        "\n",
        "\t\n",
        "\tdef apply_readout(self, x, x_size, readout_size):\n",
        "\t\treadout_weight = self.weight_variable([x_size, readout_size])\n",
        "\t\treadout_bias = self.bias_variable([readout_size])\n",
        "\t\treturn tf.add(tf.matmul(x, readout_weight), readout_bias)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbvlAhU4JxoX",
        "colab_type": "text"
      },
      "source": [
        "## function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0frxZlqJwmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def multiclass_roc_auc_score(y_true, y_score):\n",
        "    assert y_true.shape == y_score.shape\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "    n_classes = y_true.shape[1]\n",
        "    # compute ROC curve and ROC area for each class\n",
        "    for i in range(n_classes):\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_score[:, i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "    # compute micro-average ROC curve and ROC area\n",
        "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true.ravel(), y_score.ravel())\n",
        "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "    # compute macro-average ROC curve and ROC area\n",
        "    # First aggregate all false probtive rates\n",
        "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
        "    # Then interpolate all ROC curves at this points\n",
        "    mean_tpr = np.zeros_like(all_fpr)\n",
        "    for i in range(n_classes):\n",
        "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
        "    # Finally average it and compute AUC\n",
        "    mean_tpr /= n_classes\n",
        "    fpr[\"macro\"] = all_fpr\n",
        "    tpr[\"macro\"] = mean_tpr\n",
        "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
        "    return roc_auc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAM_Kj2xNL49",
        "colab_type": "text"
      },
      "source": [
        "## load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V65lzFwqKHtK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "816815b4-1fe5-40a4-9f1d-a9a2499aca98"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7ZD2h0uKbge",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "495f8369-34e3-406f-e148-9d8ed5917afe"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/braindecode_data')\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "subject_1_X_test.npy   subject_4_X_test.npy   subject_7_X_test.npy\n",
            "subject_1_X_train.npy  subject_4_X_train.npy  subject_7_X_train.npy\n",
            "subject_1_y_test.npy   subject_4_y_test.npy   subject_7_y_test.npy\n",
            "subject_1_y_train.npy  subject_4_y_train.npy  subject_7_y_train.npy\n",
            "subject_2_X_test.npy   subject_5_X_test.npy   subject_8_X_test.npy\n",
            "subject_2_X_train.npy  subject_5_X_train.npy  subject_8_X_train.npy\n",
            "subject_2_y_test.npy   subject_5_y_test.npy   subject_8_y_test.npy\n",
            "subject_2_y_train.npy  subject_5_y_train.npy  subject_8_y_train.npy\n",
            "subject_3_X_test.npy   subject_6_X_test.npy   subject_9_X_test.npy\n",
            "subject_3_X_train.npy  subject_6_X_train.npy  subject_9_X_train.npy\n",
            "subject_3_y_test.npy   subject_6_y_test.npy   subject_9_y_test.npy\n",
            "subject_3_y_train.npy  subject_6_y_train.npy  subject_9_y_train.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1Ztni1TKpRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #cross_subject\n",
        "# folder = 'BCI_IV'\n",
        "# X_tmp = np.load(os.path.join(folder,'X.npy'))\n",
        "# y_tmp = np.load(os.path.join(folder,'y.npy'))\n",
        "# y_tmp = np.asarray(pd.get_dummies(y_tmp.ravel()),dtype=np.int16)\n",
        "# sub = np.load(os.path.join(folder,'sub.npy'))\n",
        "# sub = sub.reshape(-1,)\n",
        "\n",
        "# sub_len = len(np.unique(sub))\n",
        "\n",
        "# #data augmenation\n",
        "\n",
        "# # print('length of the subject:')\n",
        "# # print(sub_len)\n",
        "# # X = np.expand_dims(X,axis=3)\n",
        "# # print('shape:')\n",
        "# print(X_tmp.shape, y_tmp.shape, sub.shape)\n",
        "\n",
        "# print('sub:'+str(np.unique(sub)))\n",
        "# print('y:'+str(np.unique(y_tmp)))\n",
        "\n",
        "# # cross subject\n",
        "# sub_select = 2\n",
        "# train_X = X_tmp[sub!=sub_select,:,:]\n",
        "# train_y = y_tmp[sub!=sub_select,:]\n",
        "# sub_train = sub[sub!= sub_select,]\n",
        "\n",
        "# train_X  = train_X.astype(np.float32) \n",
        "\n",
        "\n",
        "# #cross subject\n",
        "# test_X = X_tmp[sub==sub_select,:,:]\n",
        "# test_y = y_tmp[sub==sub_select,:]\n",
        "\n",
        "# test_X = test_X.astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbQaVP2080IW",
        "colab_type": "text"
      },
      "source": [
        "## choosing subject"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLEDowsN7BLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_idx = 1\n",
        "X_train = np.load('subject_'+str(test_idx)+'_X_train.npy')\n",
        "X_test = np.load('subject_'+str(test_idx)+'_X_test.npy')\n",
        "y_train = np.load('subject_'+str(test_idx)+'_y_train.npy')\n",
        "y_test = np.load('subject_'+str(test_idx)+'_y_test.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3Hx-76c9dp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_y = np.asarray(pd.get_dummies(y_train.ravel()),dtype=np.int16)\n",
        "test_y = np.asarray(pd.get_dummies(y_test.ravel()),dtype=np.int16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryDVNyjmJ7o6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e83b17b2-f3c7-4327-9006-07292db607b7"
      },
      "source": [
        "window_size = 400\n",
        "step = 50\n",
        "n_channel = 22\n",
        "\n",
        "def windows(data, size, step):\n",
        "\tstart = 0\n",
        "\twhile ((start+size) < data.shape[0]):\n",
        "\t\tyield int(start), int(start + size)\n",
        "\t\tstart += step\n",
        "\n",
        "\n",
        "def segment_signal_without_transition(data, window_size, step):\n",
        "\tsegments = []\n",
        "\tfor (start, end) in windows(data, window_size, step):\n",
        "\t\tif(len(data[start:end]) == window_size):\n",
        "\t\t\tsegments = segments + [data[start:end]]\n",
        "\treturn np.array(segments)\n",
        "\n",
        "\n",
        "def segment_dataset(X, window_size, step):\n",
        "\twin_x = []\n",
        "\tfor i in range(X.shape[0]):\n",
        "\t\twin_x = win_x + [segment_signal_without_transition(X[i], window_size, step)]\n",
        "\twin_x = np.array(win_x)\n",
        "\treturn win_x\n",
        "\n",
        "\n",
        "train_raw_x = np.transpose(X_train[:,:,:,0], [0, 2, 1])\n",
        "test_raw_x = np.transpose(X_test[:,:,:,0], [0, 2, 1])\n",
        "\n",
        "\n",
        "train_win_x = segment_dataset(train_raw_x, window_size, step)\n",
        "print(\"train_win_x shape: \", train_win_x.shape)\n",
        "test_win_x = segment_dataset(test_raw_x, window_size, step)\n",
        "print(\"test_win_x shape: \", test_win_x.shape)\n",
        "\n",
        "# [trial, window, channel, time_length]\n",
        "train_win_x = np.transpose(train_win_x, [0, 1, 3, 2])\n",
        "print(\"train_win_x shape: \", train_win_x.shape)\n",
        "test_win_x = np.transpose(test_win_x, [0, 1, 3, 2])\n",
        "print(\"test_win_x shape: \", test_win_x.shape)\n",
        "\n",
        "\n",
        "# [trial, window, channel, time_length, 1]\n",
        "train_x = np.expand_dims(train_win_x, axis = 4)\n",
        "test_x = np.expand_dims(test_win_x, axis = 4)\n",
        "\n",
        "num_timestep = train_x.shape[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_win_x shape:  (4608, 15, 400, 22)\n",
            "test_win_x shape:  (576, 15, 400, 22)\n",
            "train_win_x shape:  (4608, 15, 22, 400)\n",
            "test_win_x shape:  (576, 15, 22, 400)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xppypY3JNP78",
        "colab_type": "text"
      },
      "source": [
        "## model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92AHGs4mMFoe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "f35efcfc-ff34-45e6-f57d-6ac2982fe480"
      },
      "source": [
        "###########################################################################\n",
        "# set model parameters\n",
        "###########################################################################\n",
        "# kernel parameter\n",
        "kernel_height_1st\t= 22\n",
        "kernel_width_1st \t= 45\n",
        "\n",
        "kernel_stride\t\t= 1\n",
        "\n",
        "conv_channel_num\t= 40\n",
        "\n",
        "# pooling parameter\n",
        "pooling_height_1st \t= 1\n",
        "pooling_width_1st \t= 75\n",
        "\n",
        "pooling_stride_1st = 10\n",
        "\n",
        "# full connected parameter\n",
        "attention_size = 512\n",
        "n_hidden_state = 64\n",
        "\n",
        "###########################################################################\n",
        "# set dataset parameters\n",
        "###########################################################################\n",
        "# input channel\n",
        "input_channel_num = 1\n",
        "\n",
        "# input height \n",
        "input_height = train_x.shape[2]\n",
        "\n",
        "# input width\n",
        "input_width = train_x.shape[3]\n",
        "\n",
        "# prediction class\n",
        "num_labels = 4\n",
        "###########################################################################\n",
        "# set training parameters\n",
        "###########################################################################\n",
        "# set learning rate\n",
        "learning_rate = 1e-4\n",
        "\n",
        "# set maximum traing epochs\n",
        "training_epochs = 200\n",
        "\n",
        "# set batch size\n",
        "batch_size = 10\n",
        "\n",
        "# set dropout probability\n",
        "dropout_prob = 0.5\n",
        "\n",
        "# set train batch number per epoch\n",
        "batch_num_per_epoch = train_x.shape[0]//batch_size\n",
        "\n",
        "# instance cnn class\n",
        "padding = 'VALID'\n",
        "\n",
        "cnn_2d = cnn(padding=padding)\n",
        "\n",
        "# input placeholder\n",
        "X = tf.placeholder(tf.float32, shape=[None, input_height, input_width, input_channel_num], name = 'X')\n",
        "Y = tf.placeholder(tf.float32, shape=[None, num_labels], name = 'Y')\n",
        "train_phase = tf.placeholder(tf.bool, name = 'train_phase')\n",
        "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "\n",
        "# first CNN layer\n",
        "conv_1 = cnn_2d.apply_conv2d(X, kernel_height_1st, kernel_width_1st, input_channel_num, conv_channel_num, kernel_stride, train_phase)\n",
        "print(\"conv 1 shape: \", conv_1.get_shape().as_list())\n",
        "pool_1 = cnn_2d.apply_max_pooling(conv_1, pooling_height_1st, pooling_width_1st, pooling_stride_1st)\n",
        "print(\"pool 1 shape: \", pool_1.get_shape().as_list())\n",
        "\n",
        "pool1_shape = pool_1.get_shape().as_list()\n",
        "pool1_flat = tf.reshape(pool_1, [-1, pool1_shape[1]*pool1_shape[2]*pool1_shape[3]])\n",
        "\n",
        "fc_drop = tf.nn.dropout(pool1_flat, keep_prob)\t\n",
        "\n",
        "lstm_in = tf.reshape(fc_drop, [-1, num_timestep, pool1_shape[1]*pool1_shape[2]*pool1_shape[3]])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-043c5c07b9d5>:69: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/layers/normalization.py:327: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "conv 1 shape:  [None, 1, 356, 40]\n",
            "pool 1 shape:  [None, 1, 29, 40]\n",
            "WARNING:tensorflow:From <ipython-input-20-f82067a8777a>:74: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyjcweDxMRLB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "9b1721f1-b375-4982-eb2e-42baa3001d45"
      },
      "source": [
        "\n",
        "########################## RNN ########################\n",
        "cells = []\n",
        "for _ in range(2):\n",
        "\tcell = tf.contrib.rnn.BasicLSTMCell(n_hidden_state, forget_bias=1.0, state_is_tuple=True)\n",
        "\tcell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
        "\tcells.append(cell)\n",
        "lstm_cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
        "\n",
        "init_state = lstm_cell.zero_state(batch_size, dtype=tf.float32)\n",
        "\n",
        "# output ==> [batch, step, n_hidden_state]\n",
        "rnn_op, states = tf.nn.dynamic_rnn(lstm_cell, lstm_in, initial_state=init_state, time_major=False)\n",
        "\n",
        "########################## attention ########################\n",
        "with tf.name_scope('Attention_layer'):\n",
        "    attention_op, alphas = attention(rnn_op, attention_size, time_major = False, return_alphas=True)\n",
        "\n",
        "attention_drop = tf.nn.dropout(attention_op, keep_prob)\t\n",
        "\n",
        "########################## readout ########################\n",
        "y_ = cnn_2d.apply_readout(attention_drop, rnn_op.shape[2].value, num_labels)\n",
        "\n",
        "# probability prediction \n",
        "y_prob = tf.nn.softmax(y_, name = \"y_prob\")\n",
        "\n",
        "# class prediction \n",
        "y_pred = tf.argmax(y_prob, 1, name = \"y_pred\")\n",
        "\n",
        "########################## loss and optimizer ########################\n",
        "# cross entropy cost function\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=Y), name = 'loss')\n",
        "\n",
        "\n",
        "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "with tf.control_dependencies(update_ops):\n",
        "\t# set training SGD optimizer\n",
        "\toptimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "# get correctly predicted object\n",
        "correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(y_), 1), tf.argmax(Y, 1))\n",
        "\n",
        "########################## define accuracy ########################\n",
        "# calculate prediction accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = 'accuracy')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-21-e32f7ae78f36>:5: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-21-e32f7ae78f36>:8: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-21-e32f7ae78f36>:13: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From <ipython-input-21-e32f7ae78f36>:32: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSsnpmLK8-CL",
        "colab_type": "text"
      },
      "source": [
        "## training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgWHyJP0PDLm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "35dbc0b5-f89b-47b0-c314-be15c07d3b39"
      },
      "source": [
        "###########################################################################\n",
        "# train test and save result\n",
        "###########################################################################\n",
        "\n",
        "# run with gpu memory growth\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "best_test_acc = []\n",
        "train_loss = []\n",
        "with tf.Session(config=config) as session:\n",
        "\tsession.run(tf.global_variables_initializer())\n",
        "\tbest_acc = 0\n",
        "\tfor epoch in range(training_epochs):\n",
        "\t\tpred_test = np.array([])\n",
        "\t\ttrue_test = []\n",
        "\t\tprob_test = []\n",
        "\t\t########################## training process ########################\n",
        "\t\tfor b in range(batch_num_per_epoch):\n",
        "\t\t\toffset = (b * batch_size) % (train_y.shape[0] - batch_size) \n",
        "\t\t\tbatch_x = train_x[offset:(offset + batch_size), :, :, :, :]\n",
        "\t\t\tbatch_x = batch_x.reshape([len(batch_x)*num_timestep, n_channel, window_size, 1])\n",
        "\t\t\tbatch_y = train_y[offset:(offset + batch_size), :]\n",
        "\t\t\t_, c = session.run([optimizer, cost], feed_dict={X: batch_x, Y: batch_y, keep_prob: 1-dropout_prob, train_phase: True})\n",
        "\t\t# calculate train and test accuracy after each training epoch\n",
        "\t\tif(epoch%1 == 0):\n",
        "\t\t\ttrain_accuracy \t= np.zeros(shape=[0], dtype=float)\n",
        "\t\t\ttest_accuracy\t= np.zeros(shape=[0], dtype=float)\n",
        "\t\t\ttrain_l \t\t= np.zeros(shape=[0], dtype=float)\n",
        "\t\t\ttest_l\t\t\t= np.zeros(shape=[0], dtype=float)\n",
        "\t\t\t# calculate train accuracy after each training epoch\n",
        "\t\t\tfor i in range(batch_num_per_epoch):\n",
        "\t\t\t\t########################## prepare training data ########################\n",
        "\t\t\t\toffset = (i * batch_size) % (train_y.shape[0] - batch_size) \n",
        "\t\t\t\ttrain_batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
        "\t\t\t\ttrain_batch_x = train_batch_x.reshape([len(train_batch_x)*num_timestep, n_channel, window_size, 1])\n",
        "\t\t\t\ttrain_batch_y = train_y[offset:(offset + batch_size), :]\n",
        "\n",
        "\t\t\t\t########################## calculate training results ########################\n",
        "\t\t\t\ttrain_a, train_c = session.run([accuracy, cost], feed_dict={X: train_batch_x, Y: train_batch_y, keep_prob: 1.0, train_phase: False})\n",
        "\t\t\t\t\n",
        "\t\t\t\ttrain_l = np.append(train_l, train_c)\n",
        "\t\t\t\ttrain_accuracy = np.append(train_accuracy, train_a)\n",
        "\t\t\tprint(\"(\"+time.asctime(time.localtime(time.time()))+\") Epoch: \", epoch+1, \" Training Cost: \", np.mean(train_l), \"Training Accuracy: \", np.mean(train_accuracy))\n",
        "\t\t\ttrain_acc = train_acc + [np.mean(train_accuracy)]\n",
        "\t\t\ttrain_loss = train_loss + [np.mean(train_l)]\n",
        "\t\t\t# calculate test accuracy after each training epoch\n",
        "\t\t\tfor j in range(batch_num_per_epoch):\n",
        "\t\t\t\t########################## prepare test data ########################\n",
        "\t\t\t\toffset = (j * batch_size) % (test_y.shape[0] - batch_size) \n",
        "\t\t\t\ttest_batch_x = test_x[offset:(offset + batch_size), :, :, :]\n",
        "\t\t\t\ttest_batch_x = test_batch_x.reshape([len(test_batch_x)*num_timestep, n_channel, window_size, 1])\n",
        "\t\t\t\ttest_batch_y = test_y[offset:(offset + batch_size), :]\n",
        "\t\t\t\t\n",
        "\t\t\t\t########################## calculate test results ########################\n",
        "\t\t\t\ttest_a, test_c, prob_v, pred_v = session.run([accuracy, cost, y_prob, y_pred], feed_dict={X: test_batch_x, Y: test_batch_y, keep_prob: 1.0, train_phase: False})\n",
        "\t\t\t\t\n",
        "\t\t\t\ttest_accuracy = np.append(test_accuracy, test_a)\n",
        "\t\t\t\ttest_l = np.append(test_l, test_c)\n",
        "\t\t\t\tpred_test = np.append(pred_test, pred_v)\n",
        "\t\t\t\ttrue_test.append(test_batch_y)\n",
        "\t\t\t\tprob_test.append(prob_v)\n",
        "\t\t\tif np.mean(test_accuracy) > best_acc :\n",
        "\t\t\t\tbest_acc = np.mean(test_accuracy)\n",
        "\t\t\ttrue_test = np.array(true_test).reshape([-1, num_labels])\n",
        "\t\t\tprob_test = np.array(prob_test).reshape([-1, num_labels])\n",
        "\t\t\tauc_roc_test = multiclass_roc_auc_score(y_true=true_test, y_score=prob_test)\n",
        "\t\t\tf1 = f1_score(y_true=np.argmax(true_test, axis = 1), y_pred=pred_test, average = 'macro')\n",
        "\t\t\tprint(\"(\"+time.asctime(time.localtime(time.time()))+\") Epoch: \", epoch+1, \"Test Cost: \", np.mean(test_l), \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  \"Test Accuracy: \", np.mean(test_accuracy), \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  \"Test f1: \", f1, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  \"Test AUC: \", auc_roc_test['macro'], \"\\n\")\n",
        "   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Fri Aug 28 07:27:57 2020) Epoch:  1  Training Cost:  1.3879954861558002 Training Accuracy:  0.2639130494678798\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Fri Aug 28 07:28:06 2020) Epoch:  1 Test Cost:  1.3883017371530118 Test Accuracy:  0.2741304411350385 Test f1:  0.1622781464421421 Test AUC:  0.5114551508070527 \n",
            "\n",
            "(Fri Aug 28 07:28:27 2020) Epoch:  2  Training Cost:  1.3854779992414556 Training Accuracy:  0.2691304407365944\n",
            "(Fri Aug 28 07:28:35 2020) Epoch:  2 Test Cost:  1.387074663328088 Test Accuracy:  0.2704347886145115 Test f1:  0.16561169844519075 Test AUC:  0.517691551468064 \n",
            "\n",
            "(Fri Aug 28 07:28:57 2020) Epoch:  3  Training Cost:  1.3839342459388402 Training Accuracy:  0.28978261471442557\n",
            "(Fri Aug 28 07:29:05 2020) Epoch:  3 Test Cost:  1.3848437177098316 Test Accuracy:  0.26760870159968086 Test f1:  0.13163609252219494 Test AUC:  0.5584538967672077 \n",
            "\n",
            "(Fri Aug 28 07:29:27 2020) Epoch:  4  Training Cost:  1.3832486354786417 Training Accuracy:  0.27934783198263335\n",
            "(Fri Aug 28 07:29:35 2020) Epoch:  4 Test Cost:  1.3846649978471839 Test Accuracy:  0.24869565793677517 Test f1:  0.16795231416549788 Test AUC:  0.5543413524087308 \n",
            "\n",
            "(Fri Aug 28 07:29:56 2020) Epoch:  5  Training Cost:  1.3800625661145085 Training Accuracy:  0.3021739190687304\n",
            "(Fri Aug 28 07:30:05 2020) Epoch:  5 Test Cost:  1.3793602134870446 Test Accuracy:  0.309565223623877 Test f1:  0.21698831273806826 Test AUC:  0.655579648125357 \n",
            "\n",
            "(Fri Aug 28 07:30:26 2020) Epoch:  6  Training Cost:  1.3760878438534945 Training Accuracy:  0.27500000605764596\n",
            "(Fri Aug 28 07:30:34 2020) Epoch:  6 Test Cost:  1.3709621875182443 Test Accuracy:  0.2982608759694773 Test f1:  0.20816654570230592 Test AUC:  0.6661191937909086 \n",
            "\n",
            "(Fri Aug 28 07:30:55 2020) Epoch:  7  Training Cost:  1.3684805981490924 Training Accuracy:  0.3045652236951434\n",
            "(Fri Aug 28 07:31:04 2020) Epoch:  7 Test Cost:  1.3493193732655566 Test Accuracy:  0.3836956597702659 Test f1:  0.3320506442547444 Test AUC:  0.7122696994813579 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBDsgmjH9tGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}